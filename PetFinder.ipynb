{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PetFinder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexvishnevskiy/PetFinder/blob/master/PetFinder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUQiaNuUEtZG"
      },
      "source": [
        "!pip install pytorch-lightning timm python-box -U albumentations wandb > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRnRbEELKmU0",
        "outputId": "561bc829-5520-4612-96c1-4bfd21e60841"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OyuBaPFDFV3"
      },
      "source": [
        "old data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eaHetQ4Cl_s"
      },
      "source": [
        "!cp /content/drive/MyDrive/PetFinder/petfinder-pawpularity-score.zip .\n",
        "!unzip /content/petfinder-pawpularity-score.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg96zs90DH6N"
      },
      "source": [
        "new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z9dO31ZcmqD"
      },
      "source": [
        "!unzip /content/drive/MyDrive/PetFinder/breeds/cat_breeds.zip -d cat_breeds\n",
        "!unzip /content/drive/MyDrive/PetFinder/breeds/dog_breeds.zip -d dog_breeds\n",
        "!unzip /content/drive/MyDrive/PetFinder/breeds/dog-breed-identification.zip -d dog_breeds_2\n",
        "\n",
        "!mkdir cat_breeds_2\n",
        "!tar -xvf /content/drive/MyDrive/PetFinder/breeds/annotations.tar.gz --directory cat_breeds_2\n",
        "!tar -xvf /content/drive/MyDrive/PetFinder/breeds/images.tar.gz --directory cat_breeds_2\n",
        "\n",
        "!mkdir PetFinder_old\n",
        "!unzip /content/drive/MyDrive/PetFinder/breeds/PetFinder_old/train.csv.zip -d PetFinder_old\n",
        "!unzip /content/drive/MyDrive/PetFinder/breeds/PetFinder_old/train_images.zip -d PetFinder_old\n",
        "!cp /content/drive/MyDrive/PetFinder/breeds/PetFinder_old/breed_labels.csv -d PetFinder_old"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsZBhlP6kPbv"
      },
      "source": [
        "from pytorch_lightning import LightningDataModule, LightningModule\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning import callbacks\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import albumentations as A\n",
        "\n",
        "from timm import create_model\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import base64\n",
        "\n",
        "from box import Box\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ZiFd9NUd7ufm",
        "outputId": "5e5f880c-edd5-4824-89d9-5b21b85c670e"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pesAMUXEZrGO"
      },
      "source": [
        "Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XAc3HhukPj2"
      },
      "source": [
        "cfg = {\n",
        "    'seed': 42,\n",
        "    'dir_path': '/content/',\n",
        "    'photo_path': '/content/train',\n",
        "    'csv_path': '/content/drive/MyDrive/PetFinder/train_dogs_cats.csv',\n",
        "    'output_dir': '/content/',\n",
        "    'logger': {\n",
        "        'save_dir': '/content/drive/MyDrive/PetFinder/models',\n",
        "        'project': 'PetFinder',\n",
        "        'log_model': True,\n",
        "    },\n",
        "    'transform': {\n",
        "        'img_size': (224, 224),\n",
        "    },\n",
        "    'loader': {\n",
        "        'train': {\n",
        "            'batch_size': 32,\n",
        "            'num_workers': 4,\n",
        "            'shuffle': True,\n",
        "            'pin_memory': False,\n",
        "        },\n",
        "        'val': {\n",
        "            'batch_size': 64,\n",
        "            'num_workers': 4,\n",
        "            'pin_memory': False,\n",
        "        }\n",
        "    },\n",
        "    'train_args': {\n",
        "        'n_splits': 5,\n",
        "        'epoch': 20,\n",
        "    },\n",
        "    'model': {\n",
        "        'name': 'swin_base_patch4_window7_224',\n",
        "        'alias_name': 'swin_base_bn_fc',\n",
        "        'freeze_layers': 0,\n",
        "        'dropout_backbone': 0,\n",
        "        'dropout_fc': 0.1,\n",
        "        'output_dim': 1\n",
        "    },\n",
        "    'loss': {\n",
        "        'module': 'nn.BCEWithLogitsLoss',\n",
        "        'alias': 'bce',\n",
        "    },\n",
        "    'optimizer':{\n",
        "        'name': 'optim.AdamW',\n",
        "        'params':{\n",
        "            'lr': 1e-5,\n",
        "            'weight_decay': 1e-3,\n",
        "        },\n",
        "    },\n",
        "    'scheduler':{\n",
        "        'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n",
        "        'params':{\n",
        "            'T_0': 7,\n",
        "            'eta_min': 1e-6\n",
        "        },\n",
        "    },\n",
        "    'trainer': {\n",
        "        'gpus': 1,\n",
        "        'accumulate_grad_batches': 2,\n",
        "        'auto_lr_find': True,\n",
        "        'progress_bar_refresh_rate': 3,\n",
        "        'fast_dev_run': False,\n",
        "        'num_sanity_val_steps': 2,\n",
        "        #'overfit_batches': 1,\n",
        "        'resume_from_checkpoint': None,\n",
        "    },\n",
        "    'results_callback': {\n",
        "        'n_images': 30,\n",
        "    },\n",
        "}\n",
        "\n",
        "cfg = Box(cfg)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XbJJ11xIzVa",
        "outputId": "ad6a51b0-8347-4fc3-8323-4c8dcfa2dd74"
      },
      "source": [
        "seed_everything(cfg.seed, workers=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgM_whiFIObt"
      },
      "source": [
        "val_transforms = lambda img_size: A.Compose([\n",
        "     A.Resize(*img_size),\n",
        "     A.Normalize(\n",
        "         mean = [0.485, 0.456, 0.406],\n",
        "         std = [0.229, 0.224, 0.225],\n",
        "         always_apply = True\n",
        "         ),\n",
        "     ToTensorV2(),\n",
        "     ])\n",
        "\n",
        "train_transforms = lambda img_size: A.Compose([\n",
        "     A.HorizontalFlip(p = 0.5),\n",
        "     A.VerticalFlip(p = 0.4),\n",
        "     A.RandomBrightnessContrast(p=0.3),\n",
        "     #A.ShiftScaleRotate(p=0.2),\n",
        "     #A.RandomResizedCrop(*img_size, scale = (0.7, 1)),\n",
        "     A.Resize(*img_size),\n",
        "     A.Normalize(\n",
        "         mean = [0.485, 0.456, 0.406],\n",
        "         std = [0.229, 0.224, 0.225],\n",
        "         always_apply = True\n",
        "         ),\n",
        "     ToTensorV2(),                                \n",
        "])\n",
        "\n",
        "cfg.transform.train_transforms = train_transforms(cfg.transform.img_size)\n",
        "cfg.transform.val_transforms = val_transforms(cfg.transform.img_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbyEKJlZm7qN"
      },
      "source": [
        "# df = pd.read_csv(cfg.csv_path)\n",
        "# dat_v = CustomDataset(cfg, df, 'val')\n",
        "# dat_tr = CustomDataset(cfg, df, 'train')\n",
        "# for i in range(10):\n",
        "#   fig, ax = plt.subplots(1, 2)\n",
        "#   ax[0].imshow(dat_v[i][0])\n",
        "#   ax[1].imshow(dat_tr[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjlUOHffZtp8"
      },
      "source": [
        "Model and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIzA0DclkPma"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, cfg, df: pd.DataFrame, stage: str):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.df = self.prepare_df(df)\n",
        "    self.stage = stage\n",
        "    if stage == 'train':\n",
        "      self.transforms = cfg.transform.train_transforms\n",
        "    else:\n",
        "      self.transforms = cfg.transform.val_transforms\n",
        "\n",
        "  def prepare_df(self, df):\n",
        "    if not hasattr(df, 'path'):\n",
        "      df.loc[:, 'path'] = (\n",
        "          df['Id']\n",
        "          .apply(lambda x: os.path.join(self.cfg.photo_path, f'{x}.jpg'))\n",
        "          )\n",
        "    return df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    photo_path = self.df.iloc[index]['path']\n",
        "    label = self.df.iloc[index]['Pawpularity']\n",
        "\n",
        "    #перезаписать на isinstance\n",
        "    if self.cfg.loss.alias == 'ce':\n",
        "      label = min(label//20, 4)\n",
        "    if self.cfg.loss.alias == 'bce':\n",
        "      label /= 100\n",
        "    if self.cfg.loss.alias == 'mse':\n",
        "      label = label.float()\n",
        "\n",
        "    img = self.prepare_img(photo_path, self.transforms)\n",
        "    return img, label\n",
        "\n",
        "  @staticmethod\n",
        "  def prepare_img(path: str, transforms):\n",
        "    _img = cv2.imread(path)\n",
        "    _img = cv2.cvtColor(_img, cv2.COLOR_BGR2RGB)\n",
        "    img = transforms(image=_img)['image']\n",
        "    return img"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB9mnDDCBho8"
      },
      "source": [
        "class CustomDataModule(LightningDataModule):\n",
        "  def __init__(self, cfg, train_df: pd.DataFrame, val_df: pd.DataFrame, sampler = None):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.sampler = sampler\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_split = CustomDataset(self.cfg, self.train_df, 'train')\n",
        "    return DataLoader(\n",
        "        train_split,\n",
        "        sampler = self.sampler,\n",
        "        batch_size=self.cfg.loader.train.batch_size, \n",
        "        #shuffle=self.cfg.loader.train.shuffle, \n",
        "        num_workers=self.cfg.loader.train.num_workers,\n",
        "        drop_last = True\n",
        "        )\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    val_split = CustomDataset(self.cfg, self.val_df, 'val')\n",
        "    return DataLoader(\n",
        "        val_split, \n",
        "        batch_size=self.cfg.loader.val.batch_size, \n",
        "        shuffle=False,\n",
        "        num_workers=self.cfg.loader.val.num_workers,\n",
        "        )\n",
        "    \n",
        "  def predict_dataloader(self):\n",
        "     predict_split = CustomDataset(self.cfg, self.val_df, 'val')\n",
        "     return DataLoader(\n",
        "        predict_split, \n",
        "        batch_size=self.cfg.loader.val.batch_size, \n",
        "        shuffle=False,\n",
        "        num_workers=self.cfg.loader.val.num_workers,\n",
        "        )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg2QDbsoOO8B"
      },
      "source": [
        "class CustomModel(LightningModule):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.__build_model(cfg.model.freeze_layers)\n",
        "    self._criterion = eval(self.cfg.loss.module)()\n",
        "    self.save_hyperparameters(cfg)\n",
        "\n",
        "  def __build_model(self, freeze_layers: int = 0):\n",
        "    ## add freezing of layers\n",
        "    self.backbone = self.__create_model(freeze_layers)\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Dropout(self.cfg.model.dropout_fc),\n",
        "        nn.LazyLinear(self.cfg.model.output_dim)\n",
        "    )\n",
        "  \n",
        "  def __create_model(self, freeze_layers):\n",
        "    model = create_model(\n",
        "        self.cfg.model.name, \n",
        "        drop_rate = self.cfg.model.dropout_backbone, \n",
        "        pretrained=True, \n",
        "        num_classes=0, \n",
        "        in_chans=3\n",
        "        )\n",
        "    # for p in model.layers[:freeze_layers].parameters():\n",
        "    #   p.requires_grad = False\n",
        "\n",
        "    return model\n",
        "    \n",
        "  def forward(self, x):\n",
        "    f = self.backbone(x)\n",
        "    out = self.fc(f)\n",
        "    return out\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = eval(self.cfg.optimizer.name)(\n",
        "        self.parameters(), **self.cfg.optimizer.params\n",
        "        )\n",
        "    scheduler = eval(self.cfg.scheduler.name)(\n",
        "        optimizer,\n",
        "        **self.cfg.scheduler.params\n",
        "        )\n",
        "    \n",
        "    if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "      return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}\n",
        "    return [optimizer], [scheduler]\n",
        "\n",
        "  def __share_step(self, batch, stage = 'train'):\n",
        "    img, labels = batch\n",
        "    logits = self(img).squeeze()\n",
        "    loss = self._criterion(logits, labels)\n",
        "\n",
        "    #переписать на isinstance\n",
        "    if self.cfg.loss.alias == 'bce':\n",
        "      preds = logits.sigmoid() * 100 \n",
        "      labels = labels * 100\n",
        "    if self.cfg.loss.alias == 'ce':\n",
        "      preds = (logits.argmax(dim = -1).float() + 1) * 20\n",
        "      labels = (labels + 1)* 20\n",
        "\n",
        "    return loss, preds, labels\n",
        "\n",
        "  def __share_epoch(self, outputs, stage):\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for out in outputs:\n",
        "      pred, label = out['pred'], out['labels']\n",
        "      preds.append(pred)\n",
        "      labels.append(label)\n",
        "\n",
        "    preds = torch.cat(preds).cpu().detach()\n",
        "    labels = torch.cat(labels).cpu().detach()\n",
        "    rmse = torch.sqrt(((labels - preds) ** 2).mean())\n",
        "\n",
        "    if self.cfg.loss.alias == 'ce':\n",
        "      self.log(f'{stage}_f1', f1_score(labels, preds, average = 'micro'))\n",
        "    self.log(f'{stage}_rmse', rmse)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    loss, preds, labels = self.__share_step(batch, 'train')\n",
        "    self.log('train_loss', loss)\n",
        "    return {'loss': loss, 'pred': preds, 'labels': labels}\n",
        "        \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    loss, preds, labels = self.__share_step(batch, 'val')\n",
        "    self.log('val_loss', loss)\n",
        "    return {'loss': loss, 'pred': preds, 'labels': labels}\n",
        "\n",
        "  def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "    img, labels = batch\n",
        "    return self(img)\n",
        "\n",
        "  def training_epoch_end(self, outputs):\n",
        "    self.__share_epoch(outputs, 'train')\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    self.__share_epoch(outputs, 'val')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XfhKuVbZlAh"
      },
      "source": [
        "Custom callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMTzC7St0rDK"
      },
      "source": [
        "class WandbWritter(callbacks.Callback):\n",
        "  def __init__(self, cfg, df):\n",
        "    self.cfg = cfg\n",
        "    self.wandb_table = wandb.Table(columns=[\"Pawpularity\", \"pred\", \"image\"])\n",
        "    self.loader = self.configure_loader(cfg, df)\n",
        "\n",
        "  def configure_loader(self, cfg, df):\n",
        "    image_dataset = CustomDataset(\n",
        "        cfg, \n",
        "        df.iloc[:cfg.results_callback.n_images], \n",
        "        'val'\n",
        "        )\n",
        "    image_dataloader = DataLoader(\n",
        "        image_dataset, \n",
        "        batch_size=1, \n",
        "        shuffle=False,\n",
        "        num_workers=cfg.loader.val.num_workers,\n",
        "        )\n",
        "    return image_dataloader\n",
        "\n",
        "  def on_validation_epoch_end(self, trainer, pl_module):\n",
        "    for b in self.loader:\n",
        "      img, label = b\n",
        "      pred = pl_module(img.cuda())[0]\n",
        "\n",
        "      # переписать на isinstance\n",
        "      if self.cfg.loss.alias == 'bce':\n",
        "        pred = (pred.sigmoid() * 100).detach().cpu()\n",
        "        label = (label * 100).detach().cpu()\n",
        "      if self.cfg.loss.alias == 'ce':\n",
        "        pred = ((pred.argmax(dim = -1).float() + 1) * 20).detach().cpu()\n",
        "        label = ((label + 1) * 20).detach().cpu()\n",
        "\n",
        "      self.wandb_table.add_data(label.squeeze(), pred.squeeze(), wandb.Image(img.squeeze()))\n",
        "    trainer.logger.experiment.log({'results': self.wandb_table})\n",
        "\n",
        "class CsvWritter(callbacks.BasePredictionWriter):\n",
        "  def __init__(self, cfg, fold, write_interval='epoch'):\n",
        "    super().__init__(write_interval)\n",
        "    self.cfg = cfg\n",
        "    self.fold = fold\n",
        "    self.configure_output_dir()\n",
        "        \n",
        "  def configure_output_dir(self):\n",
        "    if not os.path.exists(self.cfg.output_dir):\n",
        "        os.mkdir(self.cfg.output_dir)\n",
        "        \n",
        "  def write_on_epoch_end(\n",
        "      self, trainer, pl_module, predictions, batch_indices\n",
        "  ):\n",
        "      df = trainer.predict_dataloaders[0].dataset.df\n",
        "      pred = torch.cat(predictions[0]).squeeze().detach().cpu()\n",
        "      # переписать на isinstance\n",
        "      if self.cfg.loss.alias == 'bce':\n",
        "        pred = pred.sigmoid() * 100\n",
        "      if self.cfg.loss.alias == 'ce':\n",
        "        pred = (pred.argmax(dim = -1).float() + 1) * 20\n",
        "  \n",
        "      df['predictions'] = pred.numpy()\n",
        "      df.to_csv(os.path.join(self.cfg.output_dir, f'{self.fold}_sub.csv'), index = False)\n",
        "      print(\"prediction's done\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boZU2aoWZnke"
      },
      "source": [
        "Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VigQX8Ay-dbT"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=cfg.train_args.n_splits, shuffle=True, random_state=cfg.seed)\n",
        "df = pd.read_csv(cfg.csv_path)\n",
        "df[\"Pawpularity_class\"] = df.apply(\n",
        "    lambda x: f\"{x['Pawpularity']}_{x['class']}\", axis = 1\n",
        ")\n",
        "df['paw_bins'] = np.digitize(df['Pawpularity'], np.arange(0, 100, 10), right=False) - 1 \n",
        "df['weights'] = df['paw_bins'].map(df['paw_bins'].value_counts(normalize = True).sort_index()**(-1.))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBAC58aU2Ba7"
      },
      "source": [
        "# dict_bin = {i: {'index': None, 'query': None} for i in range(10)}\n",
        "# for bin in df['paw_bins'].unique():\n",
        "#   dict_bin[bin]['query'] = df[df['paw_bins'] == bin].sample(10)\n",
        "#   temp_index = []\n",
        "#   for j in set(df['paw_bins'].unique()) - set([bin]):\n",
        "#     temp_index.append(df[df['paw_bins'] == j].sample(1))\n",
        "#   dict_bin[bin]['index'] = pd.concat(temp_index)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVMBDLCn7IAS",
        "outputId": "0399a251-0753-4f14-95ed-947efd112a99"
      },
      "source": [
        "# from torch.nn import MarginRankingLoss\n",
        "\n",
        "# MarginRankingLoss(margin = nn.Parameter(torch.tensor(0.1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarginRankingLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKb06iYJzQXj"
      },
      "source": [
        "# index 10*10 items from 0 to 100\n",
        "# query 10*10 items from 0 to 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieyLRcR1UGzB"
      },
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity_class\"])):\n",
        "  print(f\"{'*'*100}\\n{'*'*45} fold: {fold} {'*'*46}\\n{'*'*100}\")\n",
        "\n",
        "  #model, data\n",
        "  train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
        "  sampler = WeightedRandomSampler(weights=train_df['weights'], num_samples = len(train_df))\n",
        "  datamodule = CustomDataModule(cfg, train_df, val_df, sampler)\n",
        "  model = CustomModel(cfg)\n",
        "\n",
        "  #callbacks\n",
        "  earystopping = EarlyStopping(monitor=\"val_rmse\", patience = 3)\n",
        "  lr_monitor = callbacks.LearningRateMonitor()\n",
        "  csv_writer = CsvWritter(cfg, fold)\n",
        "  wandb_writter = WandbWritter(cfg, val_df)\n",
        "  loss_checkpoint = callbacks.ModelCheckpoint(\n",
        "      dirpath = os.path.join(cfg.logger.save_dir, cfg.model.alias_name),\n",
        "      filename=f\"{cfg.model.alias_name}\",\n",
        "      monitor=\"val_rmse\",\n",
        "      save_top_k=1,\n",
        "      mode=\"min\",\n",
        "      save_last=False,\n",
        "      )\n",
        "\n",
        "  wandb_logger = WandbLogger(\n",
        "      log_model = cfg.logger.log_model,\n",
        "      reinit = True,\n",
        "      )\n",
        "  #define metrics to watch for min value\n",
        "  wandb.init(\n",
        "      name = f\"fold_{fold}_{cfg.model.alias_name}\",\n",
        "      project = cfg.logger.project,\n",
        "      group = f\"{cfg.model.alias_name}\",\n",
        "      )\n",
        "  wandb.define_metric(\"val_rmse\", summary=\"min\")\n",
        "  wandb.define_metric(\"train_rmse\", summary=\"min\")\n",
        "\n",
        "  #trainer\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=cfg.train_args.epoch,\n",
        "      logger = wandb_logger,\n",
        "      callbacks=[\n",
        "            lr_monitor, \n",
        "            loss_checkpoint, \n",
        "            earystopping, \n",
        "            csv_writer, \n",
        "            wandb_writter\n",
        "            ],\n",
        "      #deterministic=True,\n",
        "      **cfg.trainer,\n",
        "      )\n",
        "  \n",
        "  trainer.fit(model, datamodule=datamodule)\n",
        "  trainer.predict(model, datamodule=datamodule)\n",
        "  #wandb.finish()\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hhsqTr5u6ZT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfOaPjayBcn1"
      },
      "source": [
        "#Add more data to train\n",
        "more_data_df = pd.read_csv('/content/drive/MyDrive/PetFinder/new_data.csv')\n",
        "datamodule = CustomDataModule(cfg, more_data_df, more_data_df)\n",
        "\n",
        "for fold in range(5):\n",
        "  v = '' if fold == 0 else f'-v{fold}'\n",
        "  model_path = f'/content/drive/MyDrive/PetFinder/models/swin_base_aug_new_val/swin_base_aug_new_val{v}.ckpt'\n",
        "  model = CustomModel.load_from_checkpoint(model_path, cfg = cfg)\n",
        "  \n",
        "  csv_writer = CsvWritter(cfg, fold)\n",
        "  trainer = pl.Trainer(\n",
        "      callbacks=[\n",
        "            csv_writer, \n",
        "            ],\n",
        "      **cfg.trainer,\n",
        "      )\n",
        "  trainer.predict(model, datamodule=datamodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hby6DZieibgU"
      },
      "source": [
        "# train_idx, val_idx = next(iter(skf.split(df[\"Id\"], df[\"Pawpularity_class\"])))\n",
        "# train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
        "# datamodule = CustomDataModule(cfg, train_df, val_df)\n",
        "# model = CustomModel(cfg)\n",
        "# #init model\n",
        "# model.cuda()\n",
        "# batch = next(iter(datamodule.val_dataloader()))\n",
        "# model(batch[0].cuda())\n",
        "\n",
        "# trainer = pl.Trainer(\n",
        "#       max_epochs=cfg.train_args.epoch,\n",
        "#       **cfg.trainer,\n",
        "#       )\n",
        "\n",
        "# lr_finder = trainer.tuner.lr_find(model, min_lr = 1e-6, max_lr = 1e-2, datamodule=datamodule)\n",
        "\n",
        "# # Results can be found in\n",
        "# lr_finder.results\n",
        "\n",
        "# # Plot with\n",
        "# fig = lr_finder.plot(suggest=True)\n",
        "# fig.show()\n",
        "\n",
        "# # Pick point based on plot, or get suggestion\n",
        "# new_lr = lr_finder.suggestion()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKOdjmCemkoK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2EHmUuTQ-Su"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL6OfRbYPoQM"
      },
      "source": [
        "cfg.loader.train.shuffle = False\n",
        "cfg.loader.train.batch_size = 64\n",
        "cfg.loader.train.num_workers = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkeSH6gkjR_M"
      },
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity_class\"])):\n",
        "  print(f\"{'*'*100}\\n{'*'*45} fold: {fold} {'*'*46}\\n{'*'*100}\")\n",
        "\n",
        "  #model, data\n",
        "  train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
        "  train_df.index = range(len(train_df))\n",
        "  val_df.index = range(len(val_df))\n",
        "  datamodule = CustomDataModule(cfg, train_df, val_df)\n",
        "  csv_writer = CsvWritter(cfg, fold)\n",
        "\n",
        "  v = '' if fold == 0 else f'-v{fold}'\n",
        "  model_path = f'/content/drive/MyDrive/PetFinder/models/swin_base_aug_new_val/swin_base_aug_new_val{v}.ckpt'\n",
        "  model = CustomModel.load_from_checkpoint(model_path, cfg = cfg)\n",
        "\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=cfg.train_args.epoch,\n",
        "      callbacks=[\n",
        "            csv_writer\n",
        "            ],\n",
        "      #deterministic=True,\n",
        "      **cfg.trainer,\n",
        "      )\n",
        "  trainer.predict(model, datamodule=datamodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBBjeVV9REyK"
      },
      "source": [
        "subs = []\n",
        "for i in range(5):\n",
        "  sub = pd.read_csv(f'{i}_sub.csv')\n",
        "  sub['pred_bin'] = np.digitize(sub['predictions'], np.arange(0, 100, 10))\n",
        "  sub['paw_bin'] = np.digitize(sub['Pawpularity'], np.arange(0, 100, 10))\n",
        "  subs.append(sub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7chhC2FjH3IH",
        "outputId": "fab9f46b-aab5-4475-b61b-1db066549800"
      },
      "source": [
        "np.arange(0, 100, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK0C0FCbIAuF",
        "outputId": "b8b8cfb7-1259-47b0-e0d8-3d963d34915f"
      },
      "source": [
        "np.digitize(np.array([21, 31, 41]), np.arange(0, 100, 10))\n",
        ">20 <40"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDgELbezHXb4"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for i in range(5):\n",
        "  print(f\"fold: {i}\")\n",
        "  print(classification_report(subs[i]['paw_bin'], subs[i]['pred_bin']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgSZEePNQ4Vt"
      },
      "source": [
        "def add_data(df):\n",
        "    df['pred_paw_group'] = np.digitize(100*sigmoid(df['pred_paw']), np.array([0, 20, 40, 60, 80, 101]))\n",
        "    df['pred_paw_grouped'] = df['pred_paw_group'].map(df.groupby(['pred_paw_group'])['pred_paw'].mean())\n",
        "\n",
        "    df['pred_paw_breed'] = df['top_preds'].map(df.groupby(['top_preds'])['pred_paw'].mean())\n",
        "    df['pred_paw_class'] = df['class'].map(df.groupby(['class'])['pred_paw'].mean())\n",
        "    return df\n",
        "\n",
        "scores_bare = []\n",
        "scores_grouped = []\n",
        "\n",
        "for i in range(5):\n",
        "  df_train = pd.read_csv(f'{i}_sub_train.csv')\n",
        "  df_val = pd.read_csv(f'{i}_sub_val.csv')\n",
        "\n",
        "  add_data(df_train).to_csv(f'{i}_sub_train.csv')\n",
        "  add_data(df_val).to_csv(f'{i}_sub_val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpeFCV6BP5Wu"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def rmse(true, pred):\n",
        "  return np.sqrt(np.mean((pred-true)**2))\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqle1d1EWOPV"
      },
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lKQW5VRrpFZ"
      },
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.special import softmax\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HNp5cjLu_6Y"
      },
      "source": [
        "x_columns_to_drop = ['Pawpularity_class', 'path', 'Pawpularity', 'Id']\n",
        "y_columns = ['Pawpularity']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRRjG6yypGvb"
      },
      "source": [
        "scores_gbm = []\n",
        "\n",
        "for i in range(5):\n",
        "  gbm = LGBMRegressor(max_depth=3, n_estimators=10, learning_rate= 0.1)\n",
        "  df_train = pd.read_csv(f'/content/{i}_sub_train.csv')\n",
        "  df_val = pd.read_csv(f'/content/{i}_sub_val.csv')\n",
        "\n",
        "  df_train['pred_paw'] = df_train['pred_paw'].apply(lambda x: 100*sigmoid(x))\n",
        "  df_val['pred_paw'] = df_val['pred_paw'].apply(lambda x: 100*sigmoid(x))\n",
        "  df_train[[f'feature_{i}' for i in range(187)]] = softmax(df_train[[f'feature_{i}' for i in range(187)]], axis=-1)\n",
        "  df_train[['cat', 'dog']] = softmax(df_train[['cat', 'dog']], axis=-1)\n",
        "  df_val[['cat', 'dog']] = softmax(df_val[['cat', 'dog']], axis=-1)\n",
        "  df_val[[f'feature_{i}' for i in range(187)]] = softmax(df_val[[f'feature_{i}' for i in range(187)]], axis=-1)\n",
        "\n",
        "  gbm.fit(df_train.drop(columns = x_columns_to_drop), df_train[y_columns])\n",
        "  rmse_score = rmse(df_val['Pawpularity'], gbm.predict(df_val.drop(columns = x_columns_to_drop), ))\n",
        "\n",
        "  scores_gbm.append(rmse_score)\n",
        "  # save model\n",
        "  #joblib.dump(gbm, f'/content/drive/MyDrive/PetFinder/models/lgbm/lgb_{i}.pkl')\n",
        "\n",
        "#scores_gbm = np.array(scores_gbm).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHU6YJujW0Mz"
      },
      "source": [
        "df_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjDPZHEhV1fg",
        "outputId": "526f5a03-e872-4677-f6d5-594694edd8d6"
      },
      "source": [
        "np.array(scores_gbm).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17.909193240484328"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg76zhCIUeWL",
        "outputId": "031a9aed-ca7f-4fd2-865a-6c35d79703da"
      },
      "source": [
        "np.array(scores_gbm).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17.721596729255783"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6stHYzQqSq-N",
        "outputId": "fd9c3cb6-66b0-483a-d751-99f254a91554"
      },
      "source": [
        "np.array(scores_gbm).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17.748799584716398"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvlGho1yjiRn",
        "outputId": "a34f7a30-2882-463e-954e-06e4b2d69ed3"
      },
      "source": [
        "scores_gbm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18.09050279357285"
            ]
          },
          "execution_count": 314,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MXoYDQipyyz",
        "outputId": "3ea6db0f-a529-47eb-868a-f1eb62000f4f"
      },
      "source": [
        "scores_gbm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17.781984339757166"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEPkYUwxwEsf",
        "outputId": "38aa47e7-74e4-4195-ca3a-1383e8429fcb"
      },
      "source": [
        "scores_nn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17.863203038121718"
            ]
          },
          "execution_count": 242,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmAUYOHWIRoM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}